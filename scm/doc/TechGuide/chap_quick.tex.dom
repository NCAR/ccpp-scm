\chapter{Quick Start Guide}
\label{chapter: quick}

This chapter provides instructions for obtaining and compiling the CCPP SCM. The SCM code calls CCPP-compliant physics schemes through the CCPP framework code. As such, it requires the CCPP framework code and physics code, both of which are included as submodules within the SCM code. This package can be considered a simple example for an atmospheric model to interact with physics through the CCPP. 

Alternatively, if one doesn't have access or care to set up a machine with the appropriate system requirements but has a working Docker installation, it is possible to create and use a Docker container with a pre-configured computing environment with a pre-compiled model. This is also an avenue for running this software with a Windows PC. See section \ref{docker} for more information.

\section{Obtaining Code}
\label{obtaining_code}

The source code for the CCPP and SCM is provided through GitHub.com.  This release branch contains the tested and supported version for general use, while a development branch is less stable, yet contains the latest developer code. Instructions for using either option are discussed here.

\subsection{Release Code}

Clone the source using
\begin{lstlisting}[language=bash]
git clone --recursive -b v4.0.0 https://github.com/NCAR/gmtb-scm
\end{lstlisting}
             Recall that the \execout{recursive} option in this command clones the main gmtb-scm repository and all subrepositories (ccpp-physics and ccpp-framework). Using this option, there is no need to execute \exec{git submodule init} and \exec{git submodule update}.

The CCPP framework can be found in the ccpp/framework subdirectory at this level.  The CCPP physics parameterizations can be found in the ccpp/physics subdirectory.

\subsection{Development Code}

If you would like to contribute as a developer to this project, please see (in addition to the rest of this guide) the scientific and technical documentation included with this release:

\url{https://dtcenter.org/community-code/common-community-physics-package-ccpp/documentation}

There you will find links to all of the documentation pertinent to developers.

For working with the development branches (stability not guaranteed), check out the \exec{dtc/develop} branches of the repository (and submodules):
\begin{lstlisting}[language=bash]
git clone --recursive -b dtc/develop https://github.com/NCAR/gmtb-scm
\end{lstlisting}
You may want to double-check that the dtc/develop branch of the SCM is pointing to the latest commits of the dtc/develop branches of ccpp-physics and ccpp-framework. While we update the submodule pointers often, it is occasionally forgotten. To ensure that you have the latest development code for the submodules, execute the following:
\begin{enumerate}
\item Navigate to the ccpp-physics directory.
\begin{lstlisting}[language=bash]
cd gmtb-scm/ccpp/physics
\end{lstlisting}
\item Check out the right branch (cloning recursively as instructed above creates a ``detached head'' state for the submodules by default).
\begin{lstlisting}[language=bash]
git checkout dtc/develop
\end{lstlisting}
\item Pull down the latest changes just to be sure.
\begin{lstlisting}[language=bash]
git pull
\end{lstlisting}
\item Do the same for ccpp-framework
\begin{lstlisting}[language=bash]
cd ../framework
git checkout dtc/develop
git pull
\end{lstlisting}
\item Change back to the main directory for following the instructions in section \ref{section: compiling} assuming system requirements in section \ref{section: systemrequirements} are met.
\begin{lstlisting}[language=bash]
cd ../..
\end{lstlisting}
\end{enumerate}


\section{System Requirements, Libraries, and Tools}
\label{section: systemrequirements}

The source code for the SCM and CCPP component is in the form of programs written in FORTRAN, FORTRAN 90, and C. In addition, the I/O relies on the netCDF libraries. Beyond the standard scripts, the build system relies on use of the Python scripting language, along with cmake, GNU make and date.

The basic requirements for building and running the CCPP and SCM bundle are listed below. The versions listed reflect successful tests and there is no guarantee that the code will work with different versions.
\begin{itemize}
    \item FORTRAN 90+ compiler
    	\begin{itemize}
   	 \item ifort 18.0.5.274, 19.0.2 and 19.0.5
	 \item gfortran 6.2, 8.3, and 9.2
	 \end{itemize}
    \item C compiler
    	\begin{itemize}
	\item icc 18.0.5.274, 19.0.2 and 19.0.5
	\item gcc 6.2, 8.3, and 9.2
	\item Apple clang 11.0.0.11000033, LLVM clang 9.0.0
	\end{itemize}
    \item cmake 2.8.12.1, 2.8.12.2, 3.6.2, 3.16.3, 3.16.4
    	\begin{itemize}
	\item NOTE: Version 3.15+ is required if installing NCEPLIBS
	\end{itemize}
   	 \item netCDF 4.3.0, 4.4.0, 4.4.1.1, 4.5.0, 4.6.1, 4.6.3, 4.7.0, 4.7.3 (not 3.x) with HDF5 and ZLIB
    \item Python 2.7.5, 2.7.9, 2.7.13, and 2.7.16 (not 3.x)
\end{itemize}

Because these tools and libraries are typically the purview of system administrators to install and maintain, they are considered  part of the basic system requirements. The Unified Forecast System (UFS) release v1.0.0 of March 11, 2020, provides software packages and detailed instructions to install these prerequisites and the NCEPlibs on supported platforms (see section~\ref{section: setup_supported_platforms}).

Further, there are several utility libraries as part of the NCEPlibs package that must be installed with environment variables pointing to their locations prior to building the SCM.
\begin{itemize}
    \item bacio - Binary I/O Library
    \item sp - Spectral Transformation Library
    \item w3nco - GRIB decoder and encoder library
\end{itemize}
The following environment variables are used by the build system to properly link these libraries: \execout{BACIO\_LIB4}, \execout{SP\_LIBd}, and \execout{W3NCO\_LIBd}.  These libraries are prebuilt on most NOAA machines using the Intel compiler and on Cheyenne for the Intel and GNU compilers. The machine setup scripts mentioned in section \ref{section: compiling} load these libraries (which are identical to those used by the UFS Medium Range Weather Application on those machines) and set these environment variables for the user automatically. For installing the libraries and its prerequisites on supported platforms, existing UFS packages can be used (see section~\ref{section: setup_supported_platforms}).

\subsection{Compilers}
The CCPP and SCM have been tested on a variety of
computing platforms. Currently the CCPP system is actively supported
on Linux and MacOS computing platforms using the Intel or GNU Fortran
compilers. Please use versions listed in the previous section as unforeseen
build issues may occur when using older compiler versions. Typically the best results come from using the
most recent version of a compiler. If you have problems with compilers, please check the ``Known Issues'' section of the
release website (\url{https://dtcenter.org/community-code/common-community-physics-package-ccpp/download}).

\subsection{Installing Libraries on Supported Platforms}\label{section: setup_supported_platforms}
For users on supported platforms such as generic Linux or macOS systems, the UFS v1.0.0 release provides software packages and detailed setup instructions at \url{https://github.com/NOAA-EMC/NCEPLIBS-external/tree/ufs-v1.0.0} and \url{https://github.com/NOAA-EMC/NCEPLIBS/tree/ufs-v1.0.0}. UFS users who already installed the \execout{NCEPLIBS} package only need to set the compiler environment variables as indicated in \url{https://github.com/NOAA-EMC/NCEPLIBS-external/tree/ufs-v1.0.0/doc/README_*.txt} and source the shell script that is created by the \execout{NCEPLIBS} install process to set the required environment variables for compiling the SCM.

The SCM uses only a small part of the UFS \execout{NCEPLIBS} package and has fewer prerequisites (i.e. no \execout{ESMF} or \execout{wgrib2} needed). Users who are not planning to use the UFS can install only the minimum set of components of the \execout{NCEPLIBS-external} package (MPI, NetCDF/NetCDF-Fortran, PNG) by following the documentation in \url{https://github.com/NOAA-EMC/NCEPLIBS-external/tree/ufs-v1.0.0/doc/README_*.txt} and adding the following arguments to the \execout{cmake} call:
\begin{lstlisting}
-DBUILD_ESMF=OFF -DBUILD_JASPER=OFF -DBUILD_WGRIB2=OFF
\end{lstlisting}
Note that this approach also compiles and installs the \execout{mpich3} MPI library and the \execout{PNG} library, even though these are not required for the SCM. Alternatively, users can install NetCDF/NetCDF-Fortran manually or using the software package managers (\execout{apt}, \execout{yum}, \execout{brew}) as in \url{https://github.com/NOAA-EMC/NCEPLIBS-external/tree/ufs-v1.0.0/doc/README_*.txt}. Whatever approach is taken, users need to set the compiler enviroment variables \execout{CC},  \execout{CXX}, \execout{FC} and the environment variable \execout{NETCDF} for compiling the three NCEP libraries (instead of the \execout{NCEPLIBS} umbrella build referred to in the \execout{NCEPLIBS-external} instructions) and the SCM.

Installing the NCEP libraries: The SCM repository contains a bash installation script in \execout{gmtb-scm/contrib/build\_nceplibs.sh} that will fetch the source code of the three required NCEP libraries from their authoritative repositories on GitHub and install them locally for the SCM to use. To execute this script, perform the following step from the top level directory (\execout{gmtb-scm}). 
\begin{lstlisting}
./contrib/build_nceplibs.sh /path/to/nceplibs
\end{lstlisting}

Following successful execution of this script, the commands to set the proper environment variables mentioned above will be written to the terminal as output. One must execute the correct set for the active shell to finish the installation, e.g., for bash
\begin{lstlisting}
export BACIO_LIB4=/path/to/nceplibs/lib/libbacio_v2.2.0_4.a
export SP_LIBd=/path/to/nceplibs/lib/libsp_v2.1.0_d.a
export W3NCO_LIBd=/path/to/nceplibs/lib/libw3nco_v2.1.0_d.a
\end{lstlisting}
and for t/csh
\begin{lstlisting}
setenv BACIO_LIB4 /path/to/nceplibs/lib/libbacio_v2.2.0_4.a
setenv SP_LIBd /path/to/nceplibs/lib/libsp_v2.1.0_d.a
setenv W3NCO_LIBd /path/to/nceplibs/lib/libw3nco_v2.1.0_d.a
\end{lstlisting}

The installation of NCEPLIBS requires \execout{cmake} v3.15+. There are many ways to obtain the required version, either by following instructions provided by \execout{cmake} (\url{https://cmake.org/install/}), or by following the instructions provided for the UFS release (\url{https://github.com/NOAA-EMC/NCEPLIBS-external/tree/ufs-v1.0.0}). Prepend this installation directory of \execout{cmake} to your path environment variable to use it for building the NCEPLIBS.

\subsection{Using Existing Libraries on Preconfigured Platforms}\label{section: use_preconfigured_platforms}
Platform-specific scripts are provided to load modules and set the user environment for preconfigured platforms: These script loads compiler modules (Fortran 2003-compliant), netCDF module, python environment, etc. and set compiler and NCEPlibs environment variables. From the top-level code directory (\execout{gmtb-scm} by default), source the correct script for your platform and shell. For \textit{t/csh} shells,
\begin{lstlisting}[language=csh]
source scm/etc/Hera_setup_intel.csh
source scm/etc/Cheyenne_setup_gnu.csh
source scm/etc/Cheyenne_setup_intel.csh
\end{lstlisting}
For bourne/bash shells,
\begin{lstlisting}[language=bash]
. scm/etc/Hera_setup_intel.sh
. scm/etc/Cheyenne_setup_gnu.sh
. scm/etc/Cheyenne_setup_intel.sh
\end{lstlisting}

\section{Compiling SCM with CCPP}
\label{section: compiling}
The first step in compiling the CCPP and SCM is to properly setup your user environment as described in sections~\ref{section: setup_supported_platforms} and~\ref{section: use_preconfigured_platforms}. The second step is to download the lookup tables (large binaries, $~$324\,MB) for the Thompson microphysics package and place them in the correct directory:
From the top-level code directory (\execout{gmtb-scm} by default), execute the following script:
\begin{lstlisting}[language=bash]
./contrib/get_thompson_tables.sh
\end{lstlisting}
If the download step fails, make sure that your system's firewall does not block access to GitHub. If it does, download the file \execout{thompson\_tables.tar} from the GitHub release website using your browser and manually extract its contents in the directory \execout{scm/data/physics\_input\_data/}.

Following this step, the top level build system will use \execout{cmake} to query system parameters, execute the CCPP prebuild script to match the physics variables (between what the host model -- SCM -- can provide and what is needed by physics schemes in the CCPP), and build the physics caps needed to use them. Finally, \execout{make} is used to compile the components.
\begin{enumerate}
 \item From the top-level code directory (\execout{gmtb-scm} by default), change directory to the top-level SCM directory.
\begin{lstlisting}[language=bash]
cd scm
\end{lstlisting}
\item Make a build directory and change into it.
\begin{lstlisting}[language=bash]
mkdir bin && cd bin
\end{lstlisting}
\item Invoke \exec{cmake} on the source code to build using one of the options below.
\begin{itemize}
\item Default mode
\begin{lstlisting}[language=bash]
cmake ../src
\end{lstlisting}
\item The statements above can be modified with the following options (put before \execout{../src}):
\begin{itemize}
\item Use threading with openmp (not for macOS with clang+gfortran)
\begin{lstlisting}[language=bash]
-DOPENMP=ON
\end{lstlisting}
\item Debug mode
\begin{lstlisting}[language=bash]
-DCMAKE_BUILD_TYPE=Debug
\end{lstlisting}
\end{itemize}
\end{itemize}

CMake automatically runs the CCPP prebuild script to match required physics variables with those available from the dycore (SCM) and to generate physics caps and makefile segments. It generates software caps for each physics group defined in the supplied SDFs and generates a static library that becomes part of the SCM executable. Appropriate software caps \textbf{will be generated for all suites defined in the \execout{gmtb-scm/ccpp/suites} directory automatically.}

If necessary, the CCPP prebuild script can be executed manually from the top level directory (\execout{gmtb-scm}). The basic syntax is
\begin{lstlisting}[language=bash]
./ccpp/framework/scripts/ccpp_prebuild.py --config=./ccpp/config/ccpp_prebuild_config.py --static --suites=SCM_GFS_v15p2,SCM_GFS_v16beta,SCM_GSD_v1[...] --builddir=./scm/bin [--debug]
\end{lstlisting}
where the argument supplied via the \execout{-{}-suites} variable is a comma-separated list of suite names that exist in the \execout{./ccpp/suites} directory. Note that suite names are the suite definition filenames minus the \exec{suite\_} prefix and \exec{.xml} suffix.

\item Compile. Add \execout{VERBOSE=1} to obtain more information on the build process.
\begin{lstlisting}[language=bash]
make
\end{lstlisting}
\end{enumerate}

The resulting executable may be found at \execout{./gmtb-scm} (Full path of \execout{gmtb-scm/scm/bin/gmtb-scm}).

Although \execout{make clean} is not currently implemented, an out-of-source build is used, so all that is required to clean the build/run directory is (from the \execout{bin} directory)
\begin{lstlisting}[language=bash]
pwd #confirm that you are in the gmtb-scm/scm/bin directory before deleting files
rm -rfd *
\end{lstlisting}
Note: This command can be dangerous (deletes files without confirming), so make sure that you're in the right directory before executing!

If you encounter errors, please capture a log file from all of the steps, and contact the helpdesk at: \url{gmtb-help@ucar.edu}

\section{Run the SCM with a supplied case}
There are several test cases provided with this version of the SCM. For all cases, the SCM will go through the time steps, applying forcing and calling the physics defined in the chosen suite definition file using physics configuration options from an associated namelist. The model is executed through one of two Python run scripts that are pre-staged into the \execout{bin} directory: \execout{run\_gmtb\_scm.py} or \execout{multi\_run\_gmtb\_scm.py}. The first sets up and runs one integration while the latter will set up and run several integrations serially. 

\subsection{Single Run Script Usage} \label{subsection: singlerunscript}
Running a case requires three pieces of information: the case to run (consisting of initial conditions, geolocation, forcing data, etc.), the physics suite to use (through a CCPP suite definition file), and a physics namelist (that specifies configurable physics options to use). As discussed in chapter \ref{chapter: cases}, cases are set up via their own namelists in \execout{../etc/case\_config}. A default physics suite is provided as a user-editable variable in the script and default namelists are associated with each physics suite (through \execout{../src/default\_namelists.py}), so, technically, one must only specify a case to run with the SCM. The single run script's interface is described below.

\begin{lstlisting}[language=bash]
./run_gmtb_scm.py -c CASE_NAME [-s SUITE_NAME] [-n PHYSICS_NAMELIST_WITH_PATH] [-g] [-d]
\end{lstlisting}

When invoking the run script, the only required argument is the name of the case to run. The case name used must match one of the case configuration files located in \execout{../etc/case\_config} (\emph{without the .nml extension!}). If specifying a suite other than the default, the suite name used must match the value of the suite name in one of the suite definition files located in \execout{../../ccpp/suites} (Note: not the filename of the suite definition file). As part of the fourth CCPP release, the following suite names are valid:
\begin{enumerate}
\item SCM\_GFS\_v15p2
\item SCM\_GFS\_v16beta
\item SCM\_GFS\_v15p2\_no\_nsst
\item SCM\_GFS\_v16beta\_no\_nsst
\item SCM\_csawmg
\item SCM\_GSD\_v1
\end{enumerate}

Note that using the Thompson microphysics scheme (as in SCM\_GSD\_v1) requires the computation of look-up tables during its initialization phase. As of the release, this process has been prohibitively slow with this model, so it is HIGHLY suggested that these look-up tables are downloaded and staged to use this scheme (and the SCM\_GSD\_v1 suite) as described in section~\ref{section: compiling}.

Also note that some cases require specified surface fluxes. Special suite definition files that correspond to the suites listed above have been created and use the \execout{*\_prescribed\_surface} decoration. It is not necessary to specify this filename decoration when specifying the suite name. If the \execout{spec\_sfc\_flux} variable in the configuration file of the case being run is set to \execout{.true.}, the run script will automatically use the special suite definition file that corresponds to the chosen suite from the list above.

If specifying a namelist other than the default, the value must be an entire filename that exists in \execout{../../ccpp/physics\_namelists}. Caution should be exercised when modifying physics namelists since some redundancy between flags to control some physics parameterizations and scheme entries in the CCPP suite definition files currently exists. Values of numerical parameters are typically OK to change without fear of inconsistencies. Lastly, the \execout{-g} flag can be used to run the executable through the \exec{gdb} debugger (assuming it is installed on the system), and the \execout{-d} flag is required when running this command in a Docker container in order to successfully mount a volume between the host machine and the Docker container instance and to share the output and plots with the host machine.

A netCDF output file is generated in the location specified in the case
configuration file, if the \execout{output\_dir} variable exists in that file. Otherwise an output directory is constructed from the case, suite, and namelist used (if different from the default). All output directories are placed in the \execout{bin} directory. If using a Docker container, all output is copied to the \execout{/home} directory in container space for volume-mounting purposes. Any standard netCDF file viewing or analysis tools may be used to
examine the output file (ncdump, ncview, NCL, etc).

\subsection{Multiple Run Script Usage}\label{subsection: multirunscript}

A second Python script is provided for automating the execution of multiple integrations through repeated calling of the single run script. From the run directory, one may use this script through the following interface.

\begin{lstlisting}[language=bash]
./multi_run_gmtb_scm.py {[-c CASE_NAME] [-s SUITE_NAME] [-f PATH_TO_FILE]} [-v{v}] [-t] [-d]
\end{lstlisting}

No arguments are required for this script. The \execout{-c or --case}, \execout{-s or --suite}, or \execout{-f or --file} options form a mutually-exclusive group, so exactly one of these is allowed at one time. If \execout{--c} is specified with a case name, the script will run a set of integrations for all supported suites (defined in \execout{../src/supported\_suites.py}) for that case. If \execout{-s} is specified with a suite name, the script will run a set of integrations for all supported cases (defined in \execout{../src/supported\_cases.py}) for that that suite. If \execout{-f} is specified with the path to a filename, it will read in lists of cases, suites, and namelists to use from that file. An example for this file's syntax can be found in \execout{../src/example\_multi\_run.py}. If multiple namelists are specified in the file, there either must be one suite specified \emph{or} the number of suites must match the number of namelists. If none of the \execout{-c or --case}, \execout{-s or --suite}, or \execout{-f or --file} options group is specified, the script will run through all permutations of supported cases and suites (as defined in the files previously mentioned).

In addition to the main options, some helper options can also be used with any of those above. The \execout{-v{v} or --verbose} option can be used to output more information from the script to the console and to a log file. If this option is not used, only completion progress messages are written out. If one \execout{-v} is used, the script will write out completion progress messages and all messages and output from the single run script. If two \execout{-vv} are used, the script will also write out all messages and single run script output to a log file (\execout{multi\_run\_gmtb\_scm.log}) in the \execout{bin} directory. The option, \execout{-t or --timer}, can be used to output the elapsed time for each integration executed by the script. Note that the execution time includes file operations performed by the single run script in addition to the execution of the underlying (Fortran) SCM executable. By default, this option will execute one integration of each subprocess. Since some variability is expected for each model run, if greater precision is required, the number of integrations for timing averaging can be set through an internal script variable. This option can be useful, for example, for getting a rough idea of relative computational expense of different physics suites. Finally, the \execout{-d} flag is required when running this command in a Docker container in order to successfully mount a volume between the host machine and the Docker container instance and to share the output and plots with the host machine.

\subsection{Batch Run Script}

If using the model on HPC resources and significant amounts of processor time is anticipated for the experiments, it will likely be necessary to submit a job through the HPC's batch system. An example script has been included in the repository for running the model on Hera's batch system (SLURM). It is located in \execout{gmtb-scm/scm/etc/gmtb\_scm\_slurm\_example.py}. Edit the \execout{job\_name}, \execout{account}, etc. to suit your needs and copy to the \execout{bin} directory. The case name to be run is included in the \execout{command} variable. To use, invoke
\begin{lstlisting}[language=bash]
./gmtb_scm_slurm_example.py
\end{lstlisting}
from the \execout{bin} directory.

Additional details regarding the SCM may be found in the remainder of this guide. More information on the CCPP can be found in the CCPP Technical Documentation available at \url{https://ccpp-techdoc.readthedocs.io/en/v4.0/}.

\section{Creating and Using a Docker Container with SCM and CCPP}
\label{docker}

In order to run a precompiled version of the CCPP SCM in a container, Docker will need to be available on your machine. Please visit \url{https://www.docker.com} to download and install the version compatible with your system. Docker frequently releases updates to the software; it is recommended to apply all available updates. NOTE: In order to install Docker on your machine, you will be required to have root access privileges. More information about getting started can be found at \url{https://docs.docker.com/get-started}

The following tips were acquired during a recent installation of Docker on a machine with Windows 10 Home Edition. Further help should be obtained from your system administrator or, lacking other resources, an internet search.

\begin{itemize}
\item Windows 10 Home Edition does not support Docker Desktop due to lack of ``Hyper-V'' support, but does work with Docker Toolbox. See the installation guide (\url{https://docs.docker.com/toolbox/toolbox_install_windows/}).
\item You may need to turn on your CPU's hardware virtualization capability through your system's BIOS.
\item After a successful installation of Docker Toolbox, starting with Docker Quickstart may result in the following error even with virtualization correctly enabled: \execout{This computer doesnâ€™t have VT-X/AMD-v enabled. Enabling it in the BIOS is mandatory}. We were able to bypass this error by opening a bash terminal installed with Docker Toolbox, navigating to the directory where it was installed, and executing the following command:
\begin{lstlisting}[language=bash]
docker-machine create default --virtualbox-no-vtx-check
\end{lstlisting}
\end{itemize}

\subsection{Building the Docker image}

The Dockerfile builds CCPP SCM v4.0 from source using the GNU compiler. A number of required codes are built and installed via the DTC-supported common community container. For reference, the common community container repository can be accessed here: \url{https://github.com/NCAR/Common-Community-Container}.

The CCPP SCM has a number of system requirements and necessary libraries and tools. Below is a list, including versions, used to create the the GNU-based Docker image:
\begin{itemize}
\item gfortran - 8.3.1
\item gcc - 8.3.1
\item cmake - 3.16.5
\item netCDF - 4.6.2
\item HDF5 - 1.10.4
\item ZLIB - 1.2.7
\item SZIP - 2.1.1
\item Python - 2.7.5
\item libxml2 - 2.9.1
\item NCEPLIBS subset: bacio v2.1.0\_4, sp v2.0.2\_d, w3nco v2.0.6\_d
\end{itemize}

A Docker image containing the SCM, CCPP, and its software prerequisites can be generated from the code in the software repository obtained by following section \ref{obtaining_code} by executing the following steps:

NOTE: Windows users can execute these steps in the terminal application that was installed as part of Docker Toolbox.

\begin{enumerate}
\item Navigate to the \execout{gmtb-scm/docker} directory.
\item Run the \execout{docker build} command to generate the Docker image, using the supplied Dockerfile.
\begin{lstlisting}[language=bash]
docker build -t ccpp-scm .
\end{lstlisting}
Inspect the Dockerfile if you would like to see details for how the image is built. The image will contain SCM prerequisite software from DTC, the SCM and CCPP code, and a pre-compiled executable for the SCM with the 6 supported suites for the SCM. A successful build will show two images: dtcenter/common-community-container, and ccpp-scm. To list images, type:
\begin{lstlisting}[language=bash]
docker images
\end{lstlisting}
\end{enumerate}

\subsection{Running the Docker image}

NOTE: Windows users can execute these steps through the Docker Quickstart application installed with Docker Toolbox.

\begin{enumerate}
\item Set up a directory that will be shared between the host machine and the Docker container. When set up correctly, it will contain output generated by the SCM within the container for manipulation by the host machine. For Mac/Linux,
\begin{lstlisting}[language=bash]
mkdir -p /path/to/output
\end{lstlisting}
For Windows, you can try to create a directory of your choice to mount to the container, but it may not work, depending on your particular Docker installation. In our case, we needed to use the home directory of the current user in order to successfully mount the directory (\execout{//C/Users/my username}), so there might be no need to create a new directory.
\item Set an environment variable to use for your SCM output directory. For \textit{t/csh} shells,
\begin{lstlisting}[language=bash]
setenv OUT_DIR /path/to/output
\end{lstlisting}
For bourne/bash shells,
\begin{lstlisting}[language=bash]
export OUT_DIR=/path/to/output
\end{lstlisting}
\item To run the SCM, you can run the Docker container that was just created and give it the same run commands as discussed in sections \ref{subsection: singlerunscript} and \ref{subsection: multirunscript}. \textbf{Be sure to remember to include the \execout{-d} option for all run commands}. For example,
\begin{lstlisting}[language=bash]
docker run --rm -it -v ${OUT_DIR}:/home --name run-ccpp-scm ccpp-scm ./run_gmtb_scm.py -c twpice -d
\end{lstlisting}
will run through the TWPICE case using the default suite and namelist and put the output in the shared directory. NOTE: Windows users may need to omit the curly braces around environment variables: use \execout{\$OUT\_DIR} instead of \execout{\$\{OUT\_DIR\}}. For running through all supported cases and suites, use
\begin{lstlisting}[language=bash]
docker run --rm -it -v ${OUT_DIR}:/home --name run-ccpp-scm ccpp-scm ./multi_run_gmtb_scm.py -d
\end{lstlisting}
The options included in the above \execout{run} commands are the following:
\begin{itemize}
\item \execout{$--$rm} removes the container when it exits
\item \execout{-it} interactive mode with terminal access
\item \execout{-v} specifies the volume mount from host directory (outside container) to inside the container. Using volumes allows you to share data between the host machine and container. For running the SCM, the output is being mounted from \execout{/home} inside the container to the \execout{OUT\_DIR} on the host machine. Upon exiting the container, data mounted to the host machine will still be accessible.
\item \execout{$--$name} names the container. If no name is provided, the daemon will autogenerate a random string name.
\end{itemize}
\item To use the SCM interactively, run non-default configurations, create plots, or even develop code, issue the following command:
\begin{lstlisting}[language=bash]
docker run --rm -it -v ${OUT_DIR}:/home --name run-ccpp-scm ccpp-scm /bin/bash
\end{lstlisting}
You will be placed within the container space and within the \execout{bin} directory of the SCM with a pre-compiled executable. At this point, one could use the run scripts as described in previous sections (remembering to include the \execout{-d} option on run scripts if output is to be shared with the host machine). To create plots, from within the \execout{bin} directory of the SCM in container space, issue the following command, with an appropriately configured \execout{*.ini} file, .i.e.
\begin{lstlisting}[language=bash]
./gmtb_scm_analysis.py twpice_all_suites.ini -d
\end{lstlisting}
NOTE: If developing, since the container is ephemeral, one should push their changes to a remote git repository to save them (i.e. a fork on GitHub.com).
\end{enumerate}



