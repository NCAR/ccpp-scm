\chapter{Quick Start Guide}
\label{chapter: quick}

This chapter provides instructions for obtaining and compiling the CCPP SCM. The SCM code calls CCPP-compliant physics schemes through the CCPP framework code. As such, it requires the CCPP framework code and physics code, both of which are included as submodules within the SCM code. This package can be considered a simple example for an atmospheric model to interact with physics through the CCPP.

Alternatively, if one doesn't have access or care to set up a machine with the appropriate system requirements but has a working Docker installation, it is possible to create and use a Docker container with a pre-configured computing environment with a pre-compiled model. This is also an avenue for running this software with a Windows PC. See section \ref{docker} for more information.

\section{Obtaining Code}
\label{obtaining_code}

The source code for the CCPP and SCM is provided through GitHub.com.  This release branch contains the tested and supported version for general use, while a development branch is less stable, yet contains the latest developer code. Instructions for using either option are discussed here.

\subsection{Release Code}

Clone the source using
\begin{lstlisting}[language=bash]
git clone --recursive -b v6.0.0 https://github.com/NCAR/ccpp-scm
\end{lstlisting}
             Recall that the \execout{recursive} option in this command clones the main ccpp-scm repository and all subrepositories (ccpp-physics and ccpp-framework). Using this option, there is no need to execute \exec{git submodule init} and \exec{git submodule update}.

The CCPP framework can be found in the ccpp/framework subdirectory at this level.  The CCPP physics parameterizations can be found in the ccpp/physics subdirectory.

\subsection{Development Code}
\label{section: development_code}

If you would like to contribute as a developer to this project, please see (in addition to the rest of this guide) the scientific and technical documentation included with this release:

\url{https://dtcenter.org/community-code/common-community-physics-package-ccpp/documentation}

There you will find links to all of the documentation pertinent to developers.

For working with the development branches (stability not guaranteed), check out the \exec{main} branch of the repository:
\begin{lstlisting}[language=bash]
git clone --recursive -b main https://github.com/NCAR/ccpp-scm
\end{lstlisting}
By using the \execout{recursive} option, it guarantees that you are checking out the commits of ccpp-physics and ccpp-framework that were tested with the latest commit of the SCM main branch. You can always retrieve the commits of the submodules that were intended to be used with a given commit of the SCM by doing the following from the top level SCM directory:
\begin{lstlisting}[language=bash]
git submodule update --init --recursive
\end{lstlisting}
You can try to use the latest commits of the ccpp-physics and ccpp-framework submodules if you wish, but this may not have been tested (i.e. SCM development may lag ccpp-physics and/or ccpp-framework development). To do so:
\begin{enumerate}
\item Navigate to the ccpp-physics directory.
\begin{lstlisting}[language=bash]
cd ccpp-scm/ccpp/physics
\end{lstlisting}
\item Check out main.
\begin{lstlisting}[language=bash]
git checkout main
\end{lstlisting}
\item Pull down the latest changes just to be sure.
\begin{lstlisting}[language=bash]
git pull
\end{lstlisting}
\item Do the same for ccpp-framework
\begin{lstlisting}[language=bash]
cd ../framework
git checkout main
git pull
\end{lstlisting}
\item Change back to the main directory for following the instructions in section \ref{section: compiling} assuming system requirements in section \ref{section: systemrequirements} are met.
\begin{lstlisting}[language=bash]
cd ../..
\end{lstlisting}
\end{enumerate}


\section{System Requirements, Libraries, and Tools}
\label{section: systemrequirements}

The source code for the SCM and CCPP components is in the form of programs written in FORTRAN, FORTRAN 90, and C. In addition, the I/O relies on the NetCDF libraries. Beyond the standard scripts, the build system relies on use of the Python scripting language, along with cmake, GNU make and date.

The following software stacks have been tested with this code. Other versions of various components will likely still work, however.

\begin{itemize}
	\item gfortran 12.1.0, gcc 12.1.0, cmake 3.23.2, NetCDF 4.7.4, Python 3.9.12
	\item GNU compilers 10.1.0, cmake 3.16.4, NetCDF 4.8.1, Python 3.7.12
	\item GNU compilers 11.1.0, cmake 3.18.2, NetCDF 4.8.1, Python 3.8.5
	\item Intel compilers 2022.0.2, cmake 3.20.1, NetCDF 4.7.4, Python 3.7.11
	\item Intel compilers 2022.1.0, cmake 3.22.0, NetCDF 4.8.1, Python 3.7.12	
\end{itemize}
	
Because these tools are typically the purview of system administrators to install and maintain, they are considered  part of the basic system requirements. The Unified Forecast System (UFS) Short-Range Weather Application release v1.0.0 of March 2021 provides software packages and detailed instructions to install these prerequisites and the hpc-stack on supported platforms (see section~\ref{section: setup_supported_platforms}).

Further, there are several utility libraries as part of the hpc-stack package that must be installed with environment variables pointing to their locations prior to building the SCM.
\begin{itemize}
    \item bacio - Binary I/O Library
    \item sp - Spectral Transformation Library
    \item w3nco - GRIB decoder and encoder library
\end{itemize}
The following environment variables are used by the build system to properly link these libraries: \execout{bacio\_ROOT}, \execout{sp\_ROOT}, and \execout{w3nco\_ROOT}. Computational platforms in which these libraries are prebuilt and installed in a central location are referred to as preconfigured platforms. Examples of preconfigured platforms are most NOAA high-performance computing machines (using the Intel compiler) and the NCAR Cheyenne system (using the Intel and GNU compilers). The machine setup scripts mentioned in section \ref{section: compiling} load these libraries (which are identical to those used by the UFS Short and Medium Range Weather Applications on those machines) and set these environment variables for the user automatically. For installing the libraries and its prerequisites on supported platforms, existing UFS packages can be used (see section~\ref{section: setup_supported_platforms}).

\subsection{Compilers}
The CCPP and SCM have been tested on a variety of
computing platforms. Currently the CCPP system is actively supported
on Linux and MacOS computing platforms using the Intel or GNU Fortran
compilers. Windows users have a path to use this software through a Docker container that uses Linux internally (see section \ref{docker}). Please use compiler versions listed in the previous section as unforeseen
build issues may occur when using older versions. Typically the best results come from using the
most recent version of a compiler. If you have problems with compilers, please check the ``Known Issues'' section of the
release website (\url{https://dtcenter.org/community-code/common-community-physics-package-ccpp/download}).

\subsection{Using Existing Libraries on Preconfigured Platforms}\label{section: use_preconfigured_platforms}
Platform-specific scripts are provided to load modules and set the user environment for preconfigured platforms. These scripts load compiler modules (Fortran 2008-compliant), the NetCDF module, Python environment, etc. and set compiler and environment variables. From the top-level code directory (\execout{ccpp-scm} by default), source the correct script for your platform and shell. For \textit{t/csh} shells,
\begin{lstlisting}[language=csh]
source scm/etc/Hera_setup_intel.csh
source scm/etc/Cheyenne_setup_gnu.csh
source scm/etc/Cheyenne_setup_intel.csh
\end{lstlisting}
For bourne/bash shells,
\begin{lstlisting}[language=bash]
. scm/etc/Hera_setup_intel.sh
. scm/etc/Cheyenne_setup_gnu.sh
. scm/etc/Cheyenne_setup_intel.sh
\end{lstlisting}


\subsection{Installing Libraries on Non-preconfigured Platforms}\label{section: setup_supported_platforms}
For users on supported platforms such as generic Linux or macOS systems that have not been preconfigured, the \execout{hpc-stack} project is suggested for installing prerequisite libraries. Visit \url{https://github.com/NOAA-EMC/hpc-stack} for instructions for installing prerequisite libraries via \execout{hpc-stack} in their docs directory. UFS users who already installed libraries via the \execout{hpc-stack} package only need to set the compiler (\execout{CC, CXX, FC}), NetCDF (\execout{NetCDF\_ROOT}), and \execout{bacio},  \execout{sp} and \execout{w3nco} (\execout{bacio\_ROOT}, \execout{sp\_ROOT}, \execout{w3nco\_ROOT}) environment variables to point to their installation paths in order to compile the SCM. 

The SCM uses only a small part of the UFS \execout{hpc-stack} package and has fewer prerequisites (i.e. no \execout{ESMF} or \execout{wgrib2} needed). Users who are not planning to use the UFS can install only NetCDF/NetCDF-Fortran manually or using the software package manager (\execout{apt}, \execout{yum}, \execout{brew}).

The Python environment must provide the \execout{f90nml} module for the SCM scripts to function. Users can test if f90nml is installed using this command in the shell:
\begin{lstlisting}
python -c "import f90nml"
\end{lstlisting}
If \execout{f90nml} is installed, this command will succeed silently, otherwise an \execout{ImportError: No module named f90nml} will be printed to screen. To install the \execout{f90nml} (v0.19) Python module, use the install method preferred for your Python environment (one of the following):
\begin{itemize}
\item
\begin{lstlisting}
easy_install f90nml==0.19
\end{lstlisting}
\end{itemize}
\begin{itemize}
\item
\begin{lstlisting}
pip install f90nml==0.19
\end{lstlisting}
\end{itemize}
\begin{itemize}
\item
\begin{lstlisting}
conda install f90nml=0.19
\end{lstlisting}
\end{itemize}

or perform the following steps to install it manually from source:
\begin{lstlisting}
cd /directory/with/write/priveleges
git clone -b v0.19 https://github.com/marshallward/f90nml
cd f90nml
python setup.py install [--prefix=/my/install/directory or --user]
\end{lstlisting}
The directory \execout{/my/install/directory} must exist and its subdirectory \execout{/my/install/directory/lib/python[version]/site-packages} (or \execout{lib64} instead of \execout{lib}, depending on the system) must be in the \execout{PYTHONPATH} environment variable.

\section{Compiling SCM with CCPP}
\label{section: compiling}
The first step in compiling the CCPP and SCM is to properly setup your user environment as described in sections~\ref{section: use_preconfigured_platforms} and~\ref{section: setup_supported_platforms}. The second step is to download the lookup tables and other large datasets (large binaries, $<$1 GB) needed by the physics schemes and place them in the correct directory:
From the top-level code directory (\execout{ccpp-scm} by default), execute the following scripts:
\begin{lstlisting}[language=bash]
./contrib/get_all_static_data.sh
./contrib/get_thompson_tables.sh
\end{lstlisting}
If the download step fails, make sure that your system's firewall does not block access to GitHub. If it does, download the files \execout{comparison\_data.tar.gz}, \execout{phyiscs\_input\_data.tar.gz}, \execout{processed\_case\_input.tar.gz}, \execout{raw\_case\_input.tar.gz} from the GitHub release website using your browser and manually extract its contents in the directory \execout{scm/data}. Similarly, do the same for \execout{thompson\_tables.tar.gz}  and \execout{MG\_INCCN\_data.tar.gz} and extract to \execout{scm/data/physics\_input\_data/}. 

Following this step, the top level build system will use \execout{cmake} to query system parameters, execute the CCPP prebuild script to match the physics variables (between what the host model -- SCM -- can provide and what is needed by physics schemes in the CCPP for the chosen suites), and build the physics caps needed to use them. Finally, \execout{make} is used to compile the components.
\begin{enumerate}
 \item From the top-level code directory (\execout{ccpp-scm} by default), change directory to the top-level SCM directory.
\begin{lstlisting}[language=bash]
cd scm
\end{lstlisting}
\item Make a build directory and change into it.
\begin{lstlisting}[language=bash]
mkdir bin && cd bin
\end{lstlisting}
\item Invoke \exec{cmake} on the source code to build using one of the options below. This step is used to identify for which suites the ccpp-framework will build caps and which suites can be run in the SCM without recompiling.
\begin{itemize}
\item Default mode
\begin{lstlisting}[language=bash]
cmake ../src
\end{lstlisting}
By default, this option uses all supported suites. The list of supported suites is controlled by \execout{scm/src/suite\_info.py}.
\item All suites mode
\begin{lstlisting}[language=bash]
cmake -DCCPP_SUITES=ALL ../src
\end{lstlisting}
All suites in \execout{scm/src/suite\_info.py}, regardless of whether they're supported, will be used. This list is typically longer for the development version of the code than for releases.
\item Selected suites mode
\begin{lstlisting}[language=bash]
cmake -DCCPP_SUITES=SCM_GFS_v16,SCM_RAP ../src
\end{lstlisting}
This only compiles the listed subset of suites (which should still have a corresponding entry in \execout{scm/src/suite\_info.py}


\item The statements above can be modified with the following options (put before \execout{../src}):
\begin{itemize}
\item Use threading with openmp (not for macOS with clang+gfortran)
\begin{lstlisting}[language=bash]
-DOPENMP=ON
\end{lstlisting}
\item Debug mode
\begin{lstlisting}[language=bash]
-DCMAKE_BUILD_TYPE=Debug
\end{lstlisting}
\end{itemize}
\item One can also save the output of this step to a log file:
\begin{lstlisting}[language=bash]
cmake [-DCMAKE_BUILD_TYPE ...] ../src 2>&1 | tee log.cmake
\end{lstlisting}
\end{itemize}

CMake automatically runs the CCPP prebuild script to match required physics variables with those available from the dycore (SCM) and to generate physics caps and makefile segments. It generates software caps for each physics group defined in the supplied Suite Definition Files (SDFs) and generates a static library that becomes part of the SCM executable.

If necessary, the CCPP prebuild script can be executed manually from the top level directory (\execout{ccpp-scm}). The basic syntax is
\begin{lstlisting}[language=bash]
./ccpp/framework/scripts/ccpp_prebuild.py --config=./ccpp/config/ccpp_prebuild_config.py --suites=SCM_GFS_v16,SCM_RAP[...] --builddir=./scm/bin [--debug]
\end{lstlisting}
where the argument supplied via the \execout{-{}-suites} variable is a comma-separated list of suite names that exist in the \execout{./ccpp/suites} directory. Note that suite names are the suite definition filenames minus the \exec{suite\_} prefix and \exec{.xml} suffix.

\item Compile. Add \execout{VERBOSE=1} to obtain more information on the build process.
\begin{lstlisting}[language=bash]
make
\end{lstlisting}
\begin{itemize}
\item One may also use more threads for compilation and/or save the output of the compilation to a log file:
\begin{lstlisting}[language=bash]
make -j4 2>&1 | tee log.make
\end{lstlisting}
\end{itemize}
\end{enumerate}

The resulting executable may be found at \execout{./scm} (Full path of \execout{ccpp-scm/scm/bin/scm}).

Although \execout{make clean} is not currently implemented, an out-of-source build is used, so all that is required to clean the build directory is (from the \execout{bin} directory)
\begin{lstlisting}[language=bash]
pwd #confirm that you are in the ccpp-scm/scm/bin directory before deleting files
rm -rfd *
\end{lstlisting}
Note: This command can be dangerous (deletes files without confirming), so make sure that you're in the right directory before executing!

If you encounter errors, please capture a log file from all of the steps, and start a thread on the support forum at: \url{https://dtcenter.org/forum/ccpp-user-support/ccpp-single-column-model}

\section{Run the SCM with a supplied case}
There are several test cases provided with this version of the SCM. For all cases, the SCM will go through the time steps, applying forcing and calling the physics defined in the chosen suite definition file using physics configuration options from an associated namelist. The model is executed through a Python run script that is pre-staged into the \execout{bin} directory: \execout{run\_scm.py}. It can be used to run one integration or several integrations serially, depending on the command line arguments supplied.

\subsection{Run Script Usage} \label{subsection: singlerunscript}
Running a case requires four pieces of information: the case to run (consisting of initial conditions, geolocation, forcing data, etc.), the physics suite to use (through a CCPP suite definition file), a physics namelist (that specifies configurable physics options to use), and a tracer configuration file. As discussed in chapter \ref{chapter: cases}, cases are set up via their own namelists in \execout{../etc/case\_config}. A default physics suite is provided as a user-editable variable in the script and default namelists and tracer configurations are associated with each physics suite (through \execout{../src/suite\_info.py}), so, technically, one must only specify a case to run with the SCM when running just one integration. For running multiple integrations at once, one need only specify one argument (\execout{-m}) which runs through all permutations of supported suites from \execout{../src/suite\_info.py} and cases from \execout{../src/supported\_cases.py}. The run script's options are described below where option abbreviations are included in brackets.

\begin{itemize}
\item \execout{-{}-case [-c]}
	\begin{itemize}
	\item \textbf{This or the \execout{-{}-multirun} option are the minimum required arguments.} The case should correspond to the name of a case in \execout{../etc/case\_config} (without the \execout{.nml} extension).
	\end{itemize}
\item \execout{-{}-suite [-s]}
	\begin{itemize}
	\item The suite should correspond to the name of a suite in \execout{../ccpp/suites} (without the \execout{.xml}) extension that was supplied in the \execout{cmake} or \execout{ccpp\_prebuild} step.
	\end{itemize}
\item \execout{-{}-namelist [-n]}
	\begin{itemize}
	\item The namelist should correspond to the name of a file in \execout{../ccpp/physics\_namelists} (WITH the \execout{.nml} extension). If this argument is omitted, the default namelist for the given suite in \execout{../src/suite\_info.py} will be used.
	\end{itemize}
\item \execout{-{}-tracers [-t]}
	\begin{itemize}
	\item The tracers file should correspond to the name of a file in \execout{../etc/tracer\_config} (WITH the \execout{.txt} extension). If this argument is omitted, the default tracer configuration for the given suite in \execout{../src/suite\_info.py} will be used.
	\end{itemize}
\item \execout{-{}-multirun [-m]}
	\begin{itemize}
	\item \textbf{This or the \execout{-{}-case} option are the minimum required arguments.} When used alone, this option runs through all permutations of supported suites from \execout{../src/suite\_info.py} and cases from \execout{../src/supported\_cases.py}. When used in conjunction with the \execout{-{}- file} option, only the runs configured in the file will be run.
	\end{itemize}
\item \execout{-{}-file [-f]}
	\begin{itemize}
	\item This option may be used in conjunction with the \execout{-{}-multirun} argument. It specifies a path and filename to a python file where multiple runs are configured.
	\end{itemize}
\item \execout{-{}-gdb [-g]}
	\begin{itemize}
	\item Use this to run the executable through the \execout{gdb} debugger (if it is installed on the system).
	\end{itemize}
\item \execout{-{}-docker [-d]}
	\begin{itemize}
	\item Use this argument when running in a docker container in order to successfully mount a volume between the host machine and the Docker container instance and to share the output and plots with the host machine.
	\end{itemize}
\item \execout{-{}-runtime}
	\begin{itemize}
	\item Use this to override the runtime provided in the case configuration namelist.
	\end{itemize}
\item \execout{-{}-runtime\_mult}
	\begin{itemize}
	\item Use this to override the runtime provided in the case configuration namelist by multiplying the runtime by the given value. This is used, for example, in regression testing to reduce total runtimes.
	\end{itemize}
\item \execout{-{}-levels [-l]}
	\begin{itemize}
	\item Use this to change the number of vertical levels.
	\end{itemize}
\item \execout{-{}-npz\_type}
	\begin{itemize}
	\item Use this to change the type of FV3 vertical grid to produce (see \execout{src/scm\_vgrid.F90} for valid values).
	\end{itemize}
\item \execout{-{}-vert\_coord\_file}
	\begin{itemize}
	\item Use this to specify the path/filename of a file containing the a\_k and b\_k coefficients for the vertical grid generation code to use.
	\end{itemize}
\item \execout{-{}-bin\_dir}
	\begin{itemize}
	\item Use this to specify the path to the build directory.
	\end{itemize}
\item \execout{-{}-run\_dir}
	\begin{itemize}
	\item Use this to specify the path to the run directory.
	\end{itemize}
\item \execout{-{}-case\_data\_dir}
	\begin{itemize}
	\item Use this to specify the path to the directory containing the case data file (useful for using the DEPHY case repository).
	\end{itemize}
\item \execout{-{}-n\_itt\_out}
	\begin{itemize}
	\item Use this to specify the period of writing instantaneous output in timesteps (if different than the default specified in the script).
	\end{itemize}
\item \execout{-{}-n\_itt\_diag}
	\begin{itemize}
	\item Use this to specify the period of writing instantaneous and time-averaged diagnostic output in timesteps (if different than the default specified in the script).
	\end{itemize}
\item \execout{-{}-timestep [-dt]}
	\begin{itemize}
	\item Use this to specify the timestep to use (if different than the default specified in \execout{../src/suite\_info.py}).
	\end{itemize}
\item \execout{-{}-verbose [-v]}
	\begin{itemize}
	\item Use this option to see additional debugging output from the run script and screen output from the executable.
	\end{itemize}
\end{itemize}

When invoking the run script, the only required argument is the name of the case to run. The case name used must match one of the case configuration files located in \execout{../etc/case\_config} (\emph{without the .nml extension!}). If specifying a suite other than the default, the suite name used must match the value of the suite name in one of the suite definition files located in \execout{../../ccpp/suites} (Note: not the filename of the suite definition file). As part of the sixth CCPP release, the following suite names are valid:
\begin{enumerate}
\item SCM\_GFS\_v16
\item SCM\_GFS\_v17p8
\item SCM\_RAP
\item SCM\_HRRR
\item SCM\_RRFS\_v1beta
\item SCM\_WoFS\_v0
\end{enumerate}

Note that using the Thompson microphysics scheme requires the computation of look-up tables during its initialization phase. As of the release, this process has been prohibitively slow with this model, so it is HIGHLY suggested that these look-up tables are downloaded and staged to use this scheme as described in section~\ref{section: compiling}. The issue appears to be machine/compiler-specific, so you may be able to produce the tables with the SCM, especially when invoking \execout{cmake} with the \execout{-DOPENMP=ON} option.

Also note that some cases require specified surface fluxes. Special suite definition files that correspond to the suites listed above have been created and use the \execout{*\_prescribed\_surface} decoration. It is not necessary to specify this filename decoration when specifying the suite name. If the \execout{spec\_sfc\_flux} variable in the configuration file of the case being run is set to \execout{.true.}, the run script will automatically use the special suite definition file that corresponds to the chosen suite from the list above.

If specifying a namelist other than the default, the value must be an entire filename that exists in \execout{../../ccpp/physics\_namelists}. Caution should be exercised when modifying physics namelists since some redundancy between flags to control some physics parameterizations and scheme entries in the CCPP suite definition files currently exists. Values of numerical parameters are typically OK to change without fear of inconsistencies. If specifying a tracer configuration other than the default, the value must be an entire filename that exists in \execout{../../scm/etc/tracer\_config}. The tracers that are used should match what the physics suite expects, lest a runtime error will result. Most of the tracers are dependent on the microphysics scheme used within the suite. The tracer names that are supported as of this release are given by the following list. Note that running without \execout{sphum}, \execout{o3mr}, and \execout{liq\_wat} may result in a runtime error in all supported suites. 

\begin{enumerate}
\item sphum
\item o3mr
\item liq\_wat
\item ice\_wat
\item rainwat
\item snowwat
\item graupel
\item hailwat
\item cld\_amt
\item water\_nc
\item ice\_nc
\item rain\_nc
\item snow\_nc
\item graupel\_nc
\item hail\_nc
\item graupel\_vol
\item hail\_vol
\item ccn\_nc
\item sgs\_tke
\item liq\_aero
\item ice\_aero
\item q\_rimef
\end{enumerate}

A NetCDF output file is generated in an output directory located named with the case and suite within the run directory. If using a Docker container, all output is copied to the \execout{/home} directory in container space for volume-mounting purposes. Any standard NetCDF file viewing or analysis tools may be used to
examine the output file (ncdump, ncview, NCL, etc).

\subsection{Batch Run Script}

If using the model on HPC resources and significant amounts of processor time is anticipated for the experiments, it will likely be necessary to submit a job through the HPC's batch system. An example script has been included in the repository for running the model on Hera's batch system (SLURM). It is located in \execout{ccpp-scm/scm/etc/scm\_slurm\_example.py}. Edit the \execout{job\_name}, \execout{account}, etc. to suit your needs and copy to the \execout{bin} directory. The case name to be run is included in the \execout{command} variable. To use, invoke
\begin{lstlisting}[language=bash]
./scm_slurm_example.py
\end{lstlisting}
from the \execout{bin} directory.

Additional details regarding the SCM may be found in the remainder of this guide. More information on the CCPP can be found in the CCPP Technical Documentation available at \url{https://ccpp-techdoc.readthedocs.io/en/v6.0.0/}.

\section{Creating and Using a Docker Container with SCM and CCPP}
\label{docker}

In order to run a precompiled version of the CCPP SCM in a container, Docker will need to be available on your machine. Please visit \url{https://www.docker.com} to download and install the version compatible with your system. Docker frequently releases updates to the software; it is recommended to apply all available updates. NOTE: In order to install Docker on your machine, you will be required to have root access privileges. More information about getting started can be found at \url{https://docs.docker.com/get-started}

The following tips were acquired during a recent installation of Docker on a machine with Windows 10 Home Edition. Further help should be obtained from your system administrator or, lacking other resources, an internet search.

\begin{itemize}
\item Windows 10 Home Edition does not support Docker Desktop due to lack of ``Hyper-V'' support, but does work with Docker Toolbox. See the installation guide (\url{https://docs.docker.com/toolbox/toolbox_install_windows/}).
\item You may need to turn on your CPU's hardware virtualization capability through your system's BIOS.
\item After a successful installation of Docker Toolbox, starting with Docker Quickstart may result in the following error even with virtualization correctly enabled: \execout{This computer doesnâ€™t have VT-X/AMD-v enabled. Enabling it in the BIOS is mandatory}. We were able to bypass this error by opening a bash terminal installed with Docker Toolbox, navigating to the directory where it was installed, and executing the following command:
\begin{lstlisting}[language=bash]
docker-machine create default --virtualbox-no-vtx-check
\end{lstlisting}
\end{itemize}

\subsection{Building the Docker image}

The Dockerfile builds CCPP SCM v6.0.0 from source using the GNU compiler. A number of required codes are built and installed via the DTC-supported common community container. For reference, the common community container repository can be accessed here: \url{https://github.com/NCAR/Common-Community-Container}.

The CCPP SCM has a number of system requirements and necessary libraries and tools. Below is a list, including versions, used to create the the GNU-based Docker image:
\begin{itemize}
\item gfortran - 9.3
\item gcc - 9.3
\item cmake - 3.16.5
\item NetCDF - 4.6.2
\item HDF5 - 1.10.4
\item ZLIB - 1.2.7
\item SZIP - 2.1.1
\item Python - 3
\item NCEPLIBS subset: bacio v2.4.1\_4, sp v2.3.3\_d, w3nco v2.4.1\_d
\end{itemize}

A Docker image containing the SCM, CCPP, and its software prerequisites can be generated from the code in the software repository obtained by following section \ref{obtaining_code} by executing the following steps:

NOTE: Windows users can execute these steps in the terminal application that was installed as part of Docker Toolbox.

\begin{enumerate}
\item Navigate to the \execout{ccpp-scm/docker} directory.
\item Run the \execout{docker build} command to generate the Docker image, using the supplied Dockerfile.
\begin{lstlisting}[language=bash]
docker build -t ccpp-scm .
\end{lstlisting}
Inspect the Dockerfile if you would like to see details for how the image is built. The image will contain SCM prerequisite software from DTC, the SCM and CCPP code, and a pre-compiled executable for the SCM with the 6 supported suites for the SCM. A successful build will show two images: dtcenter/common-community-container, and ccpp-scm. To list images, type:
\begin{lstlisting}[language=bash]
docker images
\end{lstlisting}
\end{enumerate}

\subsection{Using a prebuilt Docker image from Dockerhub}

A prebuilt Docker image for this release is available on Dockerhub if it is not desired to build from source. In order to use this, execute the following from the terminal where Docker is run:
\begin{lstlisting}[language=bash]
docker pull dtcenter/ccpp-scm:v6.0.0
\end{lstlisting}
To verify that it exists afterward, run
\begin{lstlisting}[language=bash]
docker images
\end{lstlisting}

\subsection{Running the Docker image}

NOTE: Windows users can execute these steps through the Docker Quickstart application installed with Docker Toolbox.

\begin{enumerate}
\item Set up a directory that will be shared between the host machine and the Docker container. When set up correctly, it will contain output generated by the SCM within the container for manipulation by the host machine. For Mac/Linux,
\begin{lstlisting}[language=bash]
mkdir -p /path/to/output
\end{lstlisting}
For Windows, you can try to create a directory of your choice to mount to the container, but it may not work or require more configuration, depending on your particular Docker installation. We have found that Docker volume mounting in Windows can be difficult to set up correctly. One method that worked for us was to create a new directory under our local user space, and specifying the volume mount as below. In addition, with Docker Toolbox, double check that the mounted directory has correct permissions. For example, open VirtualBox, right click on the running virtual machine, and choose ``Settings''. In the dialog that appears, make sure that the directory you're trying to share shows up in ``Shared Folders" (and add it if it does not) and make sure that the ``auto-mount" and ``permanent" options are checked.
\item Set an environment variable to use for your SCM output directory. For \textit{t/csh} shells,
\begin{lstlisting}[language=bash]
setenv OUT_DIR /path/to/output
\end{lstlisting}
For bourne/bash shells,
\begin{lstlisting}[language=bash]
export OUT_DIR=/path/to/output
\end{lstlisting}
For Windows, the format that worked for us followed this example: \execout{/c/Users/my username/path/to/directory/to/mount}
\item To run the SCM, you can run the Docker container that was just created and give it the same run commands as discussed in section \ref{subsection: singlerunscript}. \textbf{Be sure to remember to include the \execout{-d} option for all run commands}. For example,
\begin{lstlisting}[language=bash]
docker run --rm -it -v ${OUT_DIR}:/home --name run-ccpp-scm ccpp-scm ./run_scm.py -c twpice -d
\end{lstlisting}
will run through the TWPICE case using the default suite and namelist and put the output in the shared directory. NOTE: Windows users may need to omit the curly braces around environment variables: use \execout{\$OUT\_DIR} instead of \execout{\$\{OUT\_DIR\}}. For running through all supported cases and suites, use
\begin{lstlisting}[language=bash]
docker run --rm -it -v ${OUT_DIR}:/home --name run-ccpp-scm ccpp-scm ./run_scm.py -m -d
\end{lstlisting}
The options included in the above \execout{run} commands are the following:
\begin{itemize}
\item \execout{$--$rm} removes the container when it exits
\item \execout{-it} interactive mode with terminal access
\item \execout{-v} specifies the volume mount from host directory (outside container) to inside the container. Using volumes allows you to share data between the host machine and container. For running the SCM, the output is being mounted from \execout{/home} inside the container to the \execout{OUT\_DIR} on the host machine. Upon exiting the container, data mounted to the host machine will still be accessible.
\item \execout{$--$name} names the container. If no name is provided, the daemon will autogenerate a random string name.
\end{itemize}
NOTE: If you are using a prebuilt image from Dockerhub, substitute the name of the image that was pulled from Dockerhub in the commands above; i.e. instead of \execout{ccpp-scm} above, one would have \execout{dtcenter/ccpp-scm:v6.0.0}.
\item To use the SCM interactively, run non-default configurations, create plots, or even develop code, issue the following command:
\begin{lstlisting}[language=bash]
docker run --rm -it -v ${OUT_DIR}:/home --name run-ccpp-scm ccpp-scm /bin/bash
\end{lstlisting}
You will be placed within the container space and within the \execout{bin} directory of the SCM with a pre-compiled executable. At this point, one could use the run scripts as described in previous sections (remembering to include the \execout{-d} option on run scripts if output is to be shared with the host machine).
NOTE: If developing, since the container is ephemeral, one should push their changes to a remote git repository to save them (i.e. a fork on GitHub.com).
\end{enumerate}
